{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyPDF2\n",
    "# pip install llama-index\n",
    "\n",
    "# this code comes from the website : \"https://medium.com/mlearning-ai/build-a-chatbot-based-on-gpt-3-and-your-documents-with-python-a-step-by-step-guide-3d6054c6838e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adding the OPEN_API_KEY to the operating system\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langchain' has no attribute 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\ESIEE\\VOYAGE SINGAP 2023\\project\\test_custom_chatbot_4.ipynb Cell 3\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_4.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset, load_from_disk\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_4.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m documents \u001b[39m=\u001b[39m SimpleDirectoryReader(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mload_data()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_4.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m index \u001b[39m=\u001b[39m GPTVectorStoreIndex\u001b[39m.\u001b[39;49mfrom_documents(documents)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_4.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# index.save_to_disk('index_chatbot_4')\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_4.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_4.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Bonus (Not required in the code, but for optimisation)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_4.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Read the index.json later on for doing the indexation only once (#MoneySaving)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_4.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_4.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# save index to disk\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_4.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m index\u001b[39m.\u001b[39mset_index_id(\u001b[39m\"\u001b[39m\u001b[39mindex_chatbot_4\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\llama_index\\indices\\base.py:85\u001b[0m, in \u001b[0;36mBaseGPTIndex.from_documents\u001b[1;34m(cls, documents, storage_context, service_context, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39m\"\"\"Create index from documents.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m storage_context \u001b[39m=\u001b[39m storage_context \u001b[39mor\u001b[39;00m StorageContext\u001b[39m.\u001b[39mfrom_defaults()\n\u001b[1;32m---> 85\u001b[0m service_context \u001b[39m=\u001b[39m service_context \u001b[39mor\u001b[39;00m ServiceContext\u001b[39m.\u001b[39;49mfrom_defaults()\n\u001b[0;32m     86\u001b[0m docstore \u001b[39m=\u001b[39m storage_context\u001b[39m.\u001b[39mdocstore\n\u001b[0;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m documents:\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\llama_index\\indices\\service_context.py:83\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[1;34m(cls, llm_predictor, prompt_helper, embed_model, node_parser, llama_logger, callback_manager, chunk_size_limit)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m\"\"\"Create a ServiceContext from defaults.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39mIf an argument is specified, then use the argument value provided for that\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mparameter. If an argument is not specified, then use the default value.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \n\u001b[0;32m     81\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m callback_manager \u001b[39m=\u001b[39m callback_manager \u001b[39mor\u001b[39;00m CallbackManager([])\n\u001b[1;32m---> 83\u001b[0m llm_predictor \u001b[39m=\u001b[39m llm_predictor \u001b[39mor\u001b[39;00m LLMPredictor()\n\u001b[0;32m     84\u001b[0m llm_predictor\u001b[39m.\u001b[39mcallback_manager \u001b[39m=\u001b[39m callback_manager\n\u001b[0;32m     86\u001b[0m \u001b[39m# NOTE: the embed_model isn't used in all indices\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\llama_index\\llm_predictor\\base.py:174\u001b[0m, in \u001b[0;36mLLMPredictor.__init__\u001b[1;34m(self, llm, retry_on_throttling, cache, callback_manager)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    167\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    168\u001b[0m     llm: Optional[BaseLanguageModel] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m     callback_manager: Optional[CallbackManager] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    172\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[39m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm \u001b[39m=\u001b[39m llm \u001b[39mor\u001b[39;00m OpenAI(temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m         langchain\u001b[39m.\u001b[39mllm_cache \u001b[39m=\u001b[39m cache\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\pydantic\\main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\pydantic\\main.py:1066\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\pydantic\\fields.py:439\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.get_default\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\langchain\\llms\\base.py:26\u001b[0m, in \u001b[0;36m_get_verbosity\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_verbosity\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mreturn\u001b[39;00m langchain\u001b[39m.\u001b[39;49mverbose\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'langchain' has no attribute 'verbose'"
     ]
    }
   ],
   "source": [
    "# Import llama_index and indexing the documents stored in the data folder\n",
    "from llama_index import GPTVectorStoreIndex, Document, SimpleDirectoryReader\n",
    "from datasets import load_dataset, load_from_disk\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "# index.save_to_disk('index_chatbot_4')\n",
    "\n",
    "# Bonus (Not required in the code, but for optimisation)\n",
    "# Read the index.json later on for doing the indexation only once (#MoneySaving)\n",
    "\n",
    "# save index to disk\n",
    "index.set_index_id(\"index_chatbot_4\")\n",
    "index.storage_context.persist('index_chatbot_4')\n",
    "index = GPTVectorStoreIndex.load_from_disk('index_chatbot_4')\n",
    "\n",
    "# vIndex = load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "\n",
    "print(index.query(\"What's next for AI Research?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
