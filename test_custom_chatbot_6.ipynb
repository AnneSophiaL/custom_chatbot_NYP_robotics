{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\python\\lib\\site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in d:\\python\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\python\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\python\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\python\\lib\\site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\python\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\python\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in d:\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\python\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in d:\\python\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from torch) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\python\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in d:\\python\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\python\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\python\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\python\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\python\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PyPDF2 in d:\\python\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-docx in d:\\python\\lib\\site-packages (0.8.11)\n",
      "Requirement already satisfied: lxml>=2.3.2 in d:\\python\\lib\\site-packages (from python-docx) (4.9.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface_hub in d:\\python\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: filelock in d:\\python\\lib\\site-packages (from huggingface_hub) (3.8.0)\n",
      "Requirement already satisfied: fsspec in d:\\python\\lib\\site-packages (from huggingface_hub) (2023.5.0)\n",
      "Requirement already satisfied: requests in d:\\python\\lib\\site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\python\\lib\\site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\python\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\python\\lib\\site-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\python\\lib\\site-packages (from huggingface_hub) (23.0)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\python\\lib\\site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\python\\lib\\site-packages (from requests->huggingface_hub) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\lib\\site-packages (from requests->huggingface_hub) (2022.9.24)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install -U PyPDF2\n",
    "%pip install python-docx\n",
    "%pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import docx\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import TextDataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "# import dataset\n",
    "# from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required functions to read text from various files located in a directory. Files can be a mix of pdf, docx, or txt.\n",
    "# Functions to read different file types\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        pdf_reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "def read_word(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def read_documents_from_directory(directory):\n",
    "    combined_text = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            combined_text += read_pdf(file_path)\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            combined_text += read_word(file_path)\n",
    "        elif filename.endswith(\".txt\"):\n",
    "            combined_text += read_txt(file_path)\n",
    "        elif filename.endswith(\".json\"):\n",
    "            combined_text += read_txt(file_path)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train_chatbot function uses the combined text data to train a GPT-2 model using the provided training arguments. \n",
    "# The resulting trained model and tokenizer are then saved to a specified output directory.\n",
    "def train_chatbot(directory, model_output_path, train_fraction=0.8):\n",
    "    # Read documents from the directory\n",
    "    combined_text = read_documents_from_directory(directory)\n",
    "    combined_text = re.sub(r'\\n+', '\\n', combined_text).strip()  # Remove excess newline characters\n",
    "\n",
    "    # Split the text into training and validation sets\n",
    "    split_index = int(train_fraction * len(combined_text))\n",
    "    train_text = combined_text[:split_index]\n",
    "    val_text = combined_text[split_index:]\n",
    "\n",
    "    # Save the training and validation data as text files\n",
    "    with open(\"train.txt\", \"w\") as f:\n",
    "        f.write(train_text)\n",
    "    with open(\"val.txt\", \"w\") as f:\n",
    "        f.write(val_text)\n",
    "\n",
    "    # Set up the tokenizer and model\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")  #also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")  #also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n",
    "\n",
    " # Prepare the dataset\n",
    "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"train.txt\", block_size=128)\n",
    "    val_dataset = TextDataset(tokenizer=tokenizer, file_path=\"val.txt\", block_size=128)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Set up the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_output_path,\n",
    "        overwrite_output_dir=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=100,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_output_path)\n",
    "    \n",
    "    # Save the tokenizer\n",
    "    tokenizer.save_pretrained(model_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The generate_response function takes a trained model, tokenizer, and a prompt string as input and generates a response\n",
    "#  using the GPT-2 model.\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_length=100):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create the attention mask and pad token id\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.46MB/s]\n",
      "d:\\Python\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lilil\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.14MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 666/666 [00:00<?, ?B/s] \n",
      "Downloading pytorch_model.bin: 100%|██████████| 3.25G/3.25G [04:30<00:00, 12.0MB/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 26214400 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\ESIEE\\VOYAGE SINGAP 2023\\project\\test_custom_chatbot_6.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGenerated response:\u001b[39m\u001b[39m\"\u001b[39m, response)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     main()\n",
      "\u001b[1;32md:\\ESIEE\\VOYAGE SINGAP 2023\\project\\test_custom_chatbot_6.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model_output_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mESIEE\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mVOYAGE SINGAP 2023\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mproject\u001b[39m\u001b[39m\\\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Train the chatbot\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m train_chatbot(directory, model_output_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Load the fine-tuned model and tokenizer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39mfrom_pretrained(model_output_path)\n",
      "\u001b[1;32md:\\ESIEE\\VOYAGE SINGAP 2023\\project\\test_custom_chatbot_6.ipynb Cell 6\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m    \u001b[39m# Set up the tokenizer and model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m    tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2-large\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m#also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m    model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mgpt2-large\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m#also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Prepare the dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m    train_dataset \u001b[39m=\u001b[39m TextDataset(tokenizer\u001b[39m=\u001b[39mtokenizer, file_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain.txt\u001b[39m\u001b[39m\"\u001b[39m, block_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py:2611\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2608\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2610\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2611\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(config, \u001b[39m*\u001b[39mmodel_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2613\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[0;32m   2614\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:961\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n\u001b[0;32m    960\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m--> 961\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer \u001b[39m=\u001b[39m GPT2Model(config)\n\u001b[0;32m    962\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mn_embd, config\u001b[39m.\u001b[39mvocab_size, bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m     \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:680\u001b[0m, in \u001b[0;36mGPT2Model.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_position_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[0;32m    679\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[39m=\u001b[39mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    681\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    683\u001b[0m \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:680\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(config\u001b[39m.\u001b[39mmax_position_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[0;32m    679\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39membd_pdrop)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([GPT2Block(config, layer_idx\u001b[39m=\u001b[39;49mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    681\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[0;32m    683\u001b[0m \u001b[39m# Model parallel\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:375\u001b[0m, in \u001b[0;36mGPT2Block.__init__\u001b[1;34m(self, config, layer_idx)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrossattention \u001b[39m=\u001b[39m GPT2Attention(config, is_cross_attention\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, layer_idx\u001b[39m=\u001b[39mlayer_idx)\n\u001b[0;32m    373\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_cross_attn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(hidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_epsilon)\n\u001b[1;32m--> 375\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m GPT2MLP(inner_dim, config)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:349\u001b[0m, in \u001b[0;36mGPT2MLP.__init__\u001b[1;34m(self, intermediate_size, config)\u001b[0m\n\u001b[0;32m    347\u001b[0m embed_dim \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mhidden_size\n\u001b[0;32m    348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc \u001b[39m=\u001b[39m Conv1D(intermediate_size, embed_dim)\n\u001b[1;32m--> 349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj \u001b[39m=\u001b[39m Conv1D(embed_dim, intermediate_size)\n\u001b[0;32m    350\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact \u001b[39m=\u001b[39m ACT2FN[config\u001b[39m.\u001b[39mactivation_function]\n\u001b[0;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(config\u001b[39m.\u001b[39mresid_pdrop)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\pytorch_utils.py:96\u001b[0m, in \u001b[0;36mConv1D.__init__\u001b[1;34m(self, nf, nx)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf \u001b[39m=\u001b[39m nf\n\u001b[1;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39;49mempty(nx, nf))\n\u001b[0;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mzeros(nf))\n\u001b[0;32m     98\u001b[0m nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39mnormal_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, std\u001b[39m=\u001b[39m\u001b[39m0.02\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 26214400 bytes."
     ]
    }
   ],
   "source": [
    "# The main function is the entry point for the program. It specifies the path to the directory containing the training data \n",
    "# and the path to the output directory for the trained model and tokenizer. \n",
    "# It then trains the chatbot using the train_chatbot function and generates a response to a specified prompt using \n",
    "# the generate_response function.\n",
    "\n",
    "def main():\n",
    "    directory = \"D:\\ESIEE\\VOYAGE SINGAP 2023\\project\\data\"  # Replace with the path to your directory containing the files\n",
    "    model_output_path = \"D:\\ESIEE\\VOYAGE SINGAP 2023\\project\\output\"\n",
    "\n",
    "    # Train the chatbot\n",
    "    train_chatbot(directory, model_output_path)\n",
    "\n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_output_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_output_path)\n",
    "\n",
    "    # Test the chatbot\n",
    "    prompt = \"What is bulk metallic glass?\"  # Replace with your desired prompt\n",
    "    response = generate_response(model, tokenizer, prompt)\n",
    "    print(\"Generated response:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_length=250):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create the attention mask and pad token id\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\lilil\\AppData\\Local\\Temp\\ipykernel_19012\\3078362692.py\", line 3, in <module>\n",
      "    my_chat_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
      "  File \"d:\\Python\\lib\\site-packages\\transformers\\modeling_utils.py\", line 283, in from_pretrained\n",
      "    <Tip warning={true}>\n",
      "  File \"d:\\Python\\lib\\site-packages\\transformers\\configuration_utils.py\", line 133, in from_pretrained\n",
      "    num_beam_groups (`int`, *optional*, defaults to 1):\n",
      "  File \"d:\\Python\\lib\\site-packages\\transformers\\file_utils.py\", line 185, in cached_path\n",
      "ValueError: unable to parse D:\\ESIEE\\VOYAGE SINGAP 2023\\project\\data\\config.json as a URL or as a local path\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 812, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\ultratb.py\", line 730, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\lilil\\AppData\\Roaming\\Python\\Python310\\site-packages\\executing\\executing.py\", line 168, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "model_path = \"D:\\ESIEE\\VOYAGE SINGAP 2023\\project\\data\"\n",
    "# Load the fine-tuned model and tokenizer\n",
    "my_chat_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "my_chat_tokenizer = GPT2Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_chat_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\ESIEE\\VOYAGE SINGAP 2023\\project\\test_custom_chatbot_6.ipynb Cell 10\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSummarize Bhattiprolu\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms thesis\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Replace with your desired prompt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#prompt = \"What is the most promising future technology?\"\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m response \u001b[39m=\u001b[39m generate_response(my_chat_model, my_chat_tokenizer, prompt, max_length\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)  \u001b[39m#\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ESIEE/VOYAGE%20SINGAP%202023/project/test_custom_chatbot_6.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGenerated response:\u001b[39m\u001b[39m\"\u001b[39m, response)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_chat_model' is not defined"
     ]
    }
   ],
   "source": [
    "# In the case of the GPT-2 tokenizer, the model uses a byte-pair encoding (BPE) algorithm, which tokenizes text into subword units. \n",
    "# As a result, one word might be represented by multiple tokens.\n",
    "\n",
    "# For example, if you set max_length to 50, the generated response will be limited to 50 tokens, which could be fewer than 50 words, depending on the text.\n",
    "prompt = \"Summarize each document\"  # Replace with your desired prompt\n",
    "#prompt = \"What is the most promising future technology?\"\n",
    "response = generate_response(my_chat_model, my_chat_tokenizer, prompt, max_length=100)  #\n",
    "print(\"Generated response:\", response)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
